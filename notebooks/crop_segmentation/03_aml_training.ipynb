{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FarmVibes.AI Crop Segmentation - Azure Machine Learning Training\n",
    "\n",
    "This notebook demonstrates how to train a neural network with Azure Machine Learning (AML) to segment crops on NDVI timeseries and [Crop Data Layer](https://data.nal.usda.gov/dataset/cropscape-cropland-data-layer#:~:text=The%20Cropland%20Data%20Layer%20%28CDL%29%2C%20hosted%20on%20CropScape%2C,as%20well%20as%20boundary%2C%20water%20and%20road%20layers.) (CDL) maps provided by FarmVibes.AI platform.\n",
    "\n",
    "\n",
    "### Conda environment setup\n",
    "Before running this notebook, let's build a conda environment. If you do not have conda installed, please follow the instructions from [Conda User Guide](https://docs.conda.io/projects/conda/en/latest/user-guide/index.html). \n",
    "\n",
    "```\n",
    "$ conda env create -f ./crop_env.yaml\n",
    "$ conda activate crop-seg\n",
    "```\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning workspace setup\n",
    "Before running this notebook, please, make sure to define the subscription id, resource group, and AML workspace that will be used to run model training. If you do not have a workspace configured, please follow the instructions from [Azure ML Quickstart](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = \"<SUBSCRIPTION_ID>\"\n",
    "RESOURCE_GROUP_NAME = \"<RESOURCE_GROUP>\"\n",
    "WORKSPACE_NAME = \"<AML_WORKSPACE_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Notebook outline\n",
    "In this notebook, we will train the segmentation model using FarmVibes.AI data through AML. Our first step is to create chips/patches from the NDVI stacks and CDL maps, and upload them to an AML workspace. We will setup the AML environment and computing resource, and train the segmentation model over uploaded chips. Once trained, we export the model as an ONNX file that can be used within FarmVibes.AI cluster, as shown in the [Inference Notebook](./04_inference.ipynb). \n",
    "\n",
    "\n",
    "Below are the main libraries used for this example and other useful links:\n",
    "- **Geospatial data manipulation**:\n",
    "    - [Shapely](https://github.com/shapely/shapely) is a library for manipulating geometric shapes.\n",
    "    - [xarray](https://github.com/pydata/xarray) and the extension [rioxarray](https://github.com/corteva/rioxarray) are used for merging and visualizing predictions.\n",
    "- **Model definition, training and exportation**:\n",
    "    - [Pytorch](https://github.com/pytorch/pytorch) is used as our deep learning framework.\n",
    "    - [TorchGeo](https://github.com/microsoft/torchgeo) is a library built for training models on geospatial data. We use it to dinamically sample fixed-sized chips to train/evaluate our model. We define torchgeo dataset at notebook_lib/datasets.py\n",
    "    - [Pytorch-Lightning](https://github.com/Lightning-AI/lightning) is wrapper over pytorch to reduce boilerplate code for training and evaluating models. We define lightning modules at notebook_lib/modules.py.\n",
    "    - [onnx](https://onnx.ai/get-started.html) is a library for exporting machine learning models to a interoperable format.\n",
    "- **Azure Machine Learning**:\n",
    "    - [AzureML Python SDK](https://learn.microsoft.com/en-us/azure/machine-learning/): is a package that offers multiple ways to interact with AML environment. In this notebook, we use it to connect to the AML workspace, upload data generated by FarmVibes.AI platform, submit a training job.\n",
    "\n",
    "### Code organization \n",
    "The training code is mainly organized into:\n",
    "\n",
    "- The datasets (`notebook_lib/datasets.py`) containing the code for loading and preparing the data produced by FarmVibes.AI.\n",
    "- The lightning data module (`notebook_lib/modules.py`) contains the code for data loaders. The modules are responsible for loading NDVI and CDL rasters from FarmVibes.AI and pre-generate the chips to be uploaded to AML workspace. It also include the lightning data module employed within the AML pipeline to load and preprocess the training and validation chips. \n",
    "- The lightning module (`notebook_lib/models.py`) contains the code for running/training/evaluating the neural network: instantiating the neural network, training steps, computing metrics, and others. If you want to change the architecure, the loss, and generally how the model is trained, this is probably where you should go to. Also check [pytorch-lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/).\n",
    "- Two utility subpackages (`notebook_lib/utils.py` and `notebook_lib/constants.py`) with supporting code for monitoring the workflow execution, defining crop indexes constants, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "### Imports & Constants\n",
    "\n",
    "General and utility imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from shapely import wkt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset generation imports (FarmVibes.AI and data modules):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/anaconda3/envs/crop-seg/lib/python3.8/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/azureuser/anaconda3/envs/crop-seg/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# FarmVibes.AI client\n",
    "from vibe_core.client import get_default_vibe_client\n",
    "\n",
    "# notebook_lib imports\n",
    "from notebook_lib.modules import CropSegDataModule, save_chips_locally\n",
    "import notebook_lib.constants as constants\n",
    "\n",
    "# Dataset constants\n",
    "CHIP_SIZE = 256\n",
    "EPOCH_SIZE = 1024\n",
    "BATCH_SIZE = 32\n",
    "NDVI_STACK_BANDS = 37\n",
    "VAL_RATIO = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AML imports and training definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AML imports\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import AmlCompute, Environment, Data, Model\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "# ONNX package for inference\n",
    "import onnx\n",
    "\n",
    "\n",
    "# AML constants\n",
    "AML_ROOT_DIR = \"./aml\"\n",
    "AML_ENV_PATH = os.path.join(AML_ROOT_DIR, \"aml_env.yaml\")\n",
    "AML_DATASET_DIR = os.path.join(AML_ROOT_DIR, \"dataset\")\n",
    "AML_CODE_DIR = \"./notebook_lib\"\n",
    "\n",
    "# AML Compute Instance name and VM Size (will reuse if CI with name exists)\n",
    "AML_COMPUTE_INFO = {\n",
    "    \"name\": \"crop-seg-compute\",\n",
    "    \"size\": \"Standard_NC6\"\n",
    "}\n",
    "\n",
    "# AML Environment name and path to the conda yaml file\n",
    "AML_ENV_INFO = {\n",
    "    \"name\": \"crop-seg-env\",\n",
    "    \"path\": os.path.join(AML_ROOT_DIR, \"aml_env.yaml\")\n",
    "}\n",
    "\n",
    "# AML Chip dataset name, version, and description\n",
    "AML_DATASET_INFO = {\n",
    "    \"name\": \"dataset_crop_seg\",\n",
    "    \"version\": \"1\",\n",
    "    \"description\": \"Crop Segmentation Dataset\"\n",
    "}\n",
    "\n",
    "# Registration name and version of model that will be trained and stored in AML\n",
    "AML_MODEL_INFO = {\n",
    "    \"name\": \"crop_seg_model\",\n",
    "    \"version\": \"1\",\n",
    "    \"aml_root_path\": \"azureml://datastores/workspaceblobstore/paths/\"\n",
    "}\n",
    "\n",
    "# Training hyperparameters\n",
    "LR = 1e-3  # Learning rate\n",
    "WD = 1e-2  # Weight decay\n",
    "MAX_EPOCHS = 10  # How many epochs to train for in AML\n",
    "\n",
    "# Change the number of workers depending on the available shared memory\n",
    "NUM_WORKERS = 4 \n",
    "SHARED_MEMORY = \"16g\"\n",
    "NUM_GPUS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the dataset with FarmVibes.AI platform\n",
    "\n",
    "We will retrieve the dataset from FarmVibes.AI cache by running the dataset generation workflow once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bdbad19b0c4c7f9d4468d1182e9ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_geometry_path = \"./input_region.wkt\"\n",
    "time_range = (datetime(2020, 1, 1), datetime(2020, 12, 31))\n",
    "\n",
    "# Reading the geometry file \n",
    "with open(input_geometry_path) as f:\n",
    "    geometry = wkt.load(f)\n",
    "\n",
    "\n",
    "# Instantiate the client\n",
    "client = get_default_vibe_client()\n",
    "\n",
    "# Run the workflow\n",
    "wf_run = client.run(\"ml/dataset_generation/datagen_crop_segmentation\", \n",
    "                    \"Retrieve dataset cached outputs\",\n",
    "                    geometry=geometry, \n",
    "                    time_range=time_range\n",
    "                    )\n",
    "\n",
    "wf_run.monitor()\n",
    "\n",
    "cdl_rasters = wf_run.output[\"cdl\"]\n",
    "ndvi_rasters = wf_run.output[\"ndvi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data and generate chips\n",
    "\n",
    "With both cdl and ndvi rasters, we will use the `CropDataModule` (from `notebook_lib.modules.py`) to preprocess them and save the chips that will be uploaded to AML workspace. \n",
    "\n",
    "As each CDL map represents a single year, we will combine multiple NDVI rasters along the year, stacking in the channel dimension a number of rasters equal to `NDVI_STACK_BANDS`. In this notebook, we set `NDVI_STACK_BANDS = 37`, which means a 10-day interval between each ndvi raster of a year.\n",
    "\n",
    "The preprocess consists of stacking the NDVI rasters and upsampling the CDL maps. For this notebook, the DataLoader also splits the ROI in two disjoint regions and extracts chips within each of them for training and validation.\n",
    "\n",
    "`CropDataModule` has the following arguments:\n",
    "\n",
    "- `ndvi_rasters`: NDVI rasters generated by FarmVibes.AI workflow.\n",
    "- `cdl_rasters`: CDL maps downloaded by FarmVibes.AI workflow.\n",
    "- `ndvi_stack_bands`: how many daily NDVI maps will be stacked to be used as input for training. Default: 37\n",
    "- `img_size`: tuple that defines the size of each chip that is fed to the network. Default: (256, 256)\n",
    "- `epoch_size`: how many samples are sampled during training for one epoch (this is for the random sampler used in training). Default: 1024\n",
    "- `batch_size`: how many samples are fed to the network in a single batch. Default: 16\n",
    "- `num_workers`: how many worker processes to use in the data loader. Default: 4\n",
    "- `val_ratio`: how much of the data to separate for validation. Default: 0.2\n",
    "- `positive_indices`: which CDL indices are considered as positive samples. Crop types with a minimum of 1e5 pixels in the RoI are available in the module `notebook_lib.constants`. You can combine multiple constants by adding them (e.g., `constants.POTATO_INDEX + constants.CORN_INDEX`) Default: `constants.CROP_INDICES`\n",
    "- `train_years`: years used for training. Default: [2020]\n",
    "- `val_years`: years used for validation. Default: [2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting CDLMask CRS from EPSG:5070 to EPSG:32611\n",
      "Converting CDLMask resolution from 30.0 to 10.0\n"
     ]
    }
   ],
   "source": [
    "data = CropSegDataModule(\n",
    "    ndvi_rasters,\n",
    "    cdl_rasters,\n",
    "    ndvi_stack_bands=NDVI_STACK_BANDS,\n",
    "    img_size=(CHIP_SIZE, CHIP_SIZE),\n",
    "    epoch_size=EPOCH_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    positive_indices=constants.CROP_INDICES,\n",
    "    val_ratio=VAL_RATIO,\n",
    ")\n",
    "\n",
    "data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate the training and validation chips and save them to a local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chips_locally(data.train_dataloader(), os.path.join(AML_DATASET_DIR, \"train\"))\n",
    "save_chips_locally(data.val_dataloader(), os.path.join(AML_DATASET_DIR, \"val\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "## AML\n",
    "\n",
    "The remainder of this notebook differentiates from the [Local Training Notebook](./03_local_training.ipynb) by leveraging AML capacities to train the segmentation model based on the dataset generated so far. In this sense, we will upload the chips to an AML workspace and submit a training job.\n",
    "\n",
    "As a first step, let's log into Azure through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login --use-device-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will instantiate a MLClient, connected to the workspace in the Azure subscription: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class RegistryOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    # This will open a browser page for\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP_NAME,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the client set up, we can upload the training and validation chips stored in `AML_DATASET_DIR` to the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = Data(\n",
    "    path=AML_DATASET_DIR,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    name=AML_DATASET_INFO[\"name\"],\n",
    "    version=AML_DATASET_INFO[\"version\"]\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AML Compute is a managed-compute infrastructure that allows us to easily create a compute instance within the workspace. For information on the compute instances types and sizes available, refer to [AML Documentation](https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target).\n",
    "\n",
    "In this example, we will create an Azure Compute Cluster with GPU. If a compute instance with the same name already exists, we will reuse it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already have a cluster named crop-seg-compute, we'll reuse it as is.\n",
      "AMLCompute with name crop-seg-compute is created, the compute size is STANDARD_NC6\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    compute_resource = ml_client.compute.get(AML_COMPUTE_INFO[\"name\"])\n",
    "    print(f\"You already have a cluster named {AML_COMPUTE_INFO['name']}, we'll reuse it as is.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(\"Creating a new compute target...\")\n",
    "\n",
    "    # Let's create the Azure ML compute object with the intended parameters\n",
    "    compute_resource = AmlCompute(\n",
    "        # Name assigned to the compute cluster\n",
    "        name=AML_COMPUTE_INFO[\"name\"],\n",
    "        # Azure ML Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=AML_COMPUTE_INFO[\"size\"],\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=4,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=300,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    compute_resource = ml_client.begin_create_or_update(compute_resource).result()\n",
    "\n",
    "print(\n",
    "    f\"AMLCompute with name {compute_resource.name} is created, the compute size is {compute_resource.size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we create our compute instance, we will need to configure an execution environment with the packages required to run our training script. We provide a minimum conda environment yaml that we will use in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name crop-seg-env is registered to workspace, the environment version is 13\n"
     ]
    }
   ],
   "source": [
    "pipeline_job_env = Environment(\n",
    "    name=AML_ENV_INFO[\"name\"],\n",
    "    description=\"Custom environment for the crop segmentation pipeline\",\n",
    "    conda_file=AML_ENV_INFO[\"path\"],\n",
    "    image = \"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04\"\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now submit the training job to AML. To do so, we need to get a reference to the upload dataset and build the command with all the necessary parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_data_asset = ml_client.data.get(name=AML_DATASET_INFO[\"name\"], version=AML_DATASET_INFO[\"version\"])\n",
    "\n",
    "my_job_inputs = {\n",
    "    \"dataset\": Input(type=AssetTypes.URI_FOLDER, path=registered_data_asset.id, mode=InputOutputModes.DOWNLOAD),\n",
    "    \"ndvi_stack_bands\": NDVI_STACK_BANDS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"max_epochs\": MAX_EPOCHS,\n",
    "    \"learning_rate\": LR,\n",
    "    \"weight_decay\": WD,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"num_gpus\": NUM_GPUS\n",
    "}\n",
    "\n",
    "my_jobs_outputs = {\n",
    "    \"onnx_model_path\": Output(type=AssetTypes.URI_FILE, \n",
    "                              path = os.path.join(AML_MODEL_INFO[\"aml_root_path\"], f\"{AML_MODEL_INFO['name']}_{AML_MODEL_INFO['version']}.onnx\"), \n",
    "                              mode = \"upload\")\n",
    "}\n",
    "\n",
    "command_str = (\n",
    "    \"python aml_train_script.py \"\n",
    "    \"--dataset ${{inputs.dataset}} \"\n",
    "    \"--onnx_model_path ${{outputs.onnx_model_path}} \"\n",
    "    \"--ndvi_stack_bands ${{inputs.ndvi_stack_bands}} \"\n",
    "    \"--batch_size ${{inputs.batch_size}} \"\n",
    "    \"--max_epochs ${{inputs.max_epochs}} \"\n",
    "    \"--learning_rate ${{inputs.learning_rate}} \"\n",
    "    \"--weight_decay ${{inputs.weight_decay}} \"\n",
    "    \"--num_workers ${{inputs.num_workers}} \"\n",
    "    \"--num_gpus ${{inputs.num_gpus}}\"\n",
    ")\n",
    "\n",
    "job = command(\n",
    "    code=AML_CODE_DIR,\n",
    "    command=command_str,\n",
    "    inputs=my_job_inputs,\n",
    "    outputs=my_jobs_outputs,\n",
    "    environment=f\"{AML_ENV_INFO['name']}@latest\" ,\n",
    "    compute=AML_COMPUTE_INFO[\"name\"],\n",
    "    experiment_name=\"crop_segmentation\",\n",
    "    display_name=\"crop_segmentation\",\n",
    "    shm_size=SHARED_MEMORY\n",
    ")\n",
    "\n",
    "# submit the command\n",
    "returned_job = ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor the status of the job, MLClient provides an endpoint to inspect the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ml.azure.com/runs/quiet_library_zq0vncy8p6?wsid=/subscriptions/360f1ea9-ce0e-4441-ab8b-a18aae98809b/resourcegroups/eywa/workspaces/farmvibes-ai-dev&tid=72f988bf-86f1-41af-91ab-2d7cd011db47'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering and exporting the model after training\n",
    "\n",
    "Our code exports the model in the [Open Neural Network Exchange](https://onnx.ai/) (ONNX) format. ONNX is a open source format that represents machine learning models, both deep learning and traditional ML. It is supported by many frameworks, tools and hardware, enabling interoperability between different components easily. Exporting our trained model as an ONNX file allows us to load it and perform inference over new data under different hardware setups and even within FarmVibes.AI platform. For additional resources, refer to the [ONNX](https://onnx.ai/get-started.html) or [PyTorch](https://pytorch.org/docs/master/onnx.html) documentations.\n",
    "\n",
    "Once the training job completes, we will register the output onnx file in AML:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model = Model(\n",
    "    path=returned_job.outputs[\"onnx_model_path\"].path,\n",
    "    name=AML_MODEL_INFO[\"name\"],\n",
    "    version=AML_MODEL_INFO[\"version\"],\n",
    "    description=\"Exported ONNX model for crop segmentation\",\n",
    "    type=\"custom_model\"\n",
    ")\n",
    "\n",
    "ml_client.models.create_or_update(run_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once registered, we can download it locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.models.download(AML_MODEL_INFO[\"name\"], version=AML_MODEL_INFO[\"version\"], download_path=AML_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use ONNX checker to verify the model was exported successfully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_output_path = os.path.join(AML_ROOT_DIR, \n",
    "                                AML_MODEL_INFO[\"name\"], \n",
    "                                f\"{AML_MODEL_INFO['name']}_{AML_MODEL_INFO['version']}.onnx\"\n",
    "                                )\n",
    "\n",
    "onnx_model = onnx.load(onnx_output_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "### Next steps\n",
    "\n",
    "With the model trained and exported into an ONNX file, we recommend following to the [Inference Notebook](./04_inference.ipynb) to see how the model can be used within FarmVibes.AI cluster for segmenting new regions.\n",
    "Besides that, we also recommend checking the [Local Training Notebook](./03_local_training.ipynb), for an example on how to train the segmentation model locally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('crop-seg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4ed1382910929959896ae46210007cc63f041dd0c6059830101875a13ee5841"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
