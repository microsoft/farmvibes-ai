{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FarmVibes.AI Crop Segmentation\n",
    "\n",
    "This notebook demonstrates how to train a neural network to segment crops on NDVI timeseries and [Crop Data Layer](https://data.nal.usda.gov/dataset/cropscape-cropland-data-layer#:~:text=The%20Cropland%20Data%20Layer%20%28CDL%29%2C%20hosted%20on%20CropScape%2C,as%20well%20as%20boundary%2C%20water%20and%20road%20layers.) (CDL) maps provided by FarmVibes.AI platform.\n",
    "\n",
    "As provided, the notebook retrieves and preprocesses a region of ~5,000 kmÂ² over an 1-year period. **We recommend having at least 500 GB of disk space available. The workflow may take multiple days to run, depending on the number of workers and your VM spec.**\n",
    "\n",
    "\n",
    "### Conda environment setup\n",
    "Before running this notebook, let's build a conda environment. If you do not have conda installed, please follow the instructions from [Conda User Guide](https://docs.conda.io/projects/conda/en/latest/user-guide/index.html). \n",
    "\n",
    "```\n",
    "$ conda env create -f ./crop_env.yaml\n",
    "$ conda activate crop-seg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook outline\n",
    "The user provides a geographical region and a date range of interest, which are used as input to a FarmVibes.AI workflow that generates the dataset for this task. The workflow consists of downloading and preprocessing Sentinel-2 data, running SpaceEye to obtain cloud-free imagery, and computing daily NDVI indexes at 10m resolution. It also downloads CDL maps in the same time frame at 30m resolution, upsampling them to 10m resolution via nearest neighbor interpolation to be used as ground-truth labels.   \n",
    "\n",
    "As each CDL map represents a single year, we combine multiple NDVI rasters along the year, stacking in the channel dimension a number of rasters equal to `NDVI_STACK_BANDS`. In this notebook, we set `NDVI_STACK_BANDS = 37`, which means a 10-day interval between each ndvi raster of a year. During training, we extract chips/patches from the NDVI stacks and CDL maps.  \n",
    "\n",
    "\n",
    "Below are the main libraries used for this example and other useful links:\n",
    "- [Pytorch](https://github.com/pytorch/pytorch) is used as our deep learning framework.\n",
    "- [TorchGeo](https://github.com/microsoft/torchgeo) is a library built for training models on geospatial data. We use it to dinamically sample fixed-sized chips to train/evaluate our model. We define torchgeo dataset at lib/datasets.py\n",
    "- [Pytorch-Lightning](https://github.com/Lightning-AI/lightning) is wrapper over pytorch to reduce boilerplate code for training and evaluating models. We define lightning modules at lib/modules.py.\n",
    "- [Shapely](https://github.com/shapely/shapely) is a library for manipulating geometric shapes.\n",
    "- [Geopandas](https://github.com/geopandas/geopandas) is an extension of the popular pandas library to add support for geographic data. It uses shapely objects in its geometry column, and understands coordinate systems. We use it to visualize the training/validation RoIs. It is also quite useful for transforming geometries between different CRSs.\n",
    "- [xarray](https://github.com/pydata/xarray) and the extension [rioxarray](https://github.com/corteva/rioxarray) are used for merging and visualizing predictions.\n",
    "- [rasterio](https://github.com/rasterio/rasterio) is a library for reading and writing geospatial raster data. It is used on torchgeo and rioxarray. It is a good option when reading/writing GeoTIFFs.\n",
    "- [onnx](https://onnx.ai/get-started.html) is a library for exporting machine learning models to a interoperable format. \n",
    "\n",
    "### Code organization\n",
    "The training code is mainly organized into four parts:\n",
    "\n",
    "- The datasets (`notebook_lib/datasets.py`) inherit from torchgeo's `RasterDataset` and are used to read and preprocess the CDL and NDVI data produced by FarmVibes.AI platform. For those familiar with pytorch, these datasets are indexed by a spatiotemporal bounding box instead of integer. If you want to add new datasets, you can use these as a base (and check [torchgeo documentation](https://torchgeo.readthedocs.io/en/latest/)).\n",
    "- The lightning data module (`notebook_lib/modules.py`) contains the code for samplers and data loaders. If you want to change the data split, how to load data, which crops to use, this is probably where you should go to. Also check [pytorch-lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/).\n",
    "- The lightning module (`notebook_lib/models.py`) contains the code for running/training/evaluating the neural network: instantiating the neural network, training steps, computing metrics, and others. If you want to change the architecure, the loss, and generally how the model is trained, this is probably where you should go to. Also check [pytorch-lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/).\n",
    "- Two utility subpackages (`notebook_lib/utils.py` and `notebook_lib/constants.py`) with supporting code for monitoring the workflow execution, defining crop indexes constants, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility imports\n",
    "from datetime import datetime\n",
    "from shapely import wkt\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rioxarray as rio\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import xarray as xr\n",
    "from rioxarray.merge import merge_arrays\n",
    "\n",
    "\n",
    "# FarmVibes.AI imports\n",
    "from vibe_core.client import get_default_vibe_client\n",
    "\n",
    "# Pytorch-related imports\n",
    "import torch\n",
    "from torchgeo.datasets import BoundingBox\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import onnx\n",
    "\n",
    "# Imports for this example notebook \n",
    "from notebook_lib.utils import bbox_to_shapely, format_timestamp\n",
    "from notebook_lib.modules import CropSegDataModule\n",
    "from notebook_lib.models import SegmentationModel\n",
    "import notebook_lib.constants as constants\n",
    "\n",
    "\n",
    "##### CONSTANTS\n",
    "# FarmVibes.AI workflow name and description\n",
    "WORKFLOW_NAME = \"ml/dataset_generation/datagen_crop_segmentation\"\n",
    "RUN_NAME = \"dataset generation for crop segmentation task\"\n",
    "\n",
    "# Dataloader / Model parameters\n",
    "ROOT_DIR = \"./model_checkpoint\"\n",
    "\n",
    "# Leave empty to train\n",
    "CHPT_PATH = \"\"\n",
    "# Add path to load checkpoint\n",
    "# CHPT_PATH = \"notebooks/crop_segmentation/model_checkpoint/lightning_logs/version_0/checkpoints/epoch=9-step=320.ckpt\"\n",
    "\n",
    "CHIP_SIZE = 256\n",
    "EPOCH_SIZE = 1024\n",
    "BATCH_SIZE = 16\n",
    "NDVI_STACK_BANDS = 37\n",
    "NUM_WORKERS = 1  # Change this depending on available memory and number of cores\n",
    "\n",
    "# Training hyperparameters\n",
    "LR = 1e-3  # Learning rate\n",
    "WD = 1e-3  # Weight decay\n",
    "MAX_EPOCHS = 10  # How many epochs to train for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the dataset with FarmVibes.AI platform\n",
    "\n",
    "Let's define the region and the time range to consider for this task:\n",
    "- **Region:** FarmVibes.AI platform expects a `.wkt` file with the polygon of the ROI (an example `input_region.wkt` is already provided);\n",
    "- **Time Range:** we define the range as a tuple with two datetimes (start and end dates);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_geometry_path = \"./input_region.wkt\"\n",
    "time_range = (datetime(2020, 1, 1), datetime(2020, 12, 31))\n",
    "\n",
    "# Reading the geometry file \n",
    "with open(input_geometry_path) as f:\n",
    "    geometry = wkt.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the crop segmentation task, we will run the `ml/dataset_generation/datagen_crop_segmentation` workflow.\n",
    "\n",
    "To build the dataset, we will instantiate the FarmVibes.AI remote client and run the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the client\n",
    "client = get_default_vibe_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.document_workflow(WORKFLOW_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "wf_run = client.run(WORKFLOW_NAME, RUN_NAME, geometry=geometry, time_range=time_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wf_run` is a `VibeWorkflowRun` that holds the information about the workflow execution. A few of its important attributes:\n",
    "- `wf_run.id`: the ID of the run\n",
    "- `wf_run.status`: indicate the status of the run (pending, running, failed, or done)\n",
    "- `wf_run.workflow`: the name of the workflow being executed (i.e., `WORKFLOW_NAME`)\n",
    "- `wf_run.name`: the description provided by `WORKFLOW_DESC`\n",
    "- `wf_run.output`: the dictionary with outputs produced by the workflow, indexed by sink names\n",
    "\n",
    "In case you need to retrieve a previous workflow run, you can use `client.list_runs()` to list all existing executions and find the id of the desired run. It can be recovered by running `wf_run = client.get_run_by_id(\"ID-of-the-run\")`.\n",
    "\n",
    "We can also use the method `monitor_until_complete` from `VibeWorkflowRun` to verify the progress of each op/inner workflow of our run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_run.monitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once finished, we can access the generated outputs through `wf_run.output`.\n",
    "\n",
    "The list of outputs of the dataset generation workflow is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a specific output, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdl_rasters = wf_run.output[\"cdl\"]\n",
    "ndvi_rasters = wf_run.output[\"ndvi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data and instantiate a DataLoader\n",
    "With the NDVI rasters and CDL maps yielded by FarmVibes.AI, we will use the `CropDataModule` (from `notebook_lib.modules.py`) to preprocess them and provide a DataLoader for the training and evaluation.\n",
    "\n",
    "The preprocess consists of stacking the NDVI rasters and upsampling the CDL maps. For this notebook, the DataLoader also splits the ROI in two disjoint regions and extracts chips within each of them for training and validation.\n",
    "\n",
    "`CropDataModule` has the following arguments:\n",
    "\n",
    "- `ndvi_rasters`: NDVI rasters generated by FarmVibes.AI workflow.\n",
    "- `cdl_rasters`: CDL maps downloaded by FarmVibes.AI workflow.\n",
    "- `ndvi_stack_bands`: how many daily NDVI maps will be stacked to be used as input for training. Default: 37\n",
    "- `img_size`: tuple that defines the size of each chip that is fed to the network. Default: (256, 256)\n",
    "- `epoch_size`: how many samples are sampled during training for one epoch (this is for the random sampler used in training). Default: 1024\n",
    "- `batch_size`: how many samples are fed to the network in a single batch. Default: 16\n",
    "- `num_workers`: how many worker processes to use in the data loader. Default: 4\n",
    "- `val_ratio`: how much of the data to separate for validation. Default: 0.2\n",
    "- `positive_indices`: which CDL indices are considered as positive samples. Crop types with a minimum of 1e5 pixels in the RoI are available in the module `notebook_lib.constants`. You can combine multiple constants by adding them (e.g., `constants.POTATO_INDEX + constants.CORN_INDEX`) Default: `constants.CROP_INDICES`\n",
    "- `train_years`: years used for training. Default: [2020]\n",
    "- `val_years`: years used for validation. Default: [2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CropSegDataModule(\n",
    "    ndvi_rasters,\n",
    "    cdl_rasters,\n",
    "    ndvi_stack_bands=NDVI_STACK_BANDS,\n",
    "    img_size=(CHIP_SIZE, CHIP_SIZE),\n",
    "    epoch_size=EPOCH_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    positive_indices=constants.CROP_INDICES,\n",
    "    val_ratio=0.3,\n",
    ")\n",
    "\n",
    "data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation splits\n",
    "When generating the training and validation chips, we consider two disjoint regions within the ROI (defined by the `val_ratio` parameter of `CropSegDataModule`). During model optimization, we sample random chips withing the training ROI, whereas validation is done on chips extracted in a grid throughout the validation ROI.\n",
    "\n",
    "We can visualize the training and validation split by sampling boxes as shown below. By default, we separate 20% of the available data for validation (shown in orange), while the rest is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_roi = data.train_dataloader().sampler.roi\n",
    "val_roi = data.val_dataloader().sampler.roi\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.gca()\n",
    "gpd.GeoSeries([bbox_to_shapely(b) for b in data.train_dataloader().sampler]).boundary.plot(ax=ax, color=\"C0\")\n",
    "gpd.GeoSeries([bbox_to_shapely(b) for b in data.val_dataloader().sampler]).boundary.plot(ax=ax, color=\"C1\")\n",
    "gpd.GeoSeries(bbox_to_shapely(train_roi)).boundary.plot(ax=ax, color=\"black\")\n",
    "gpd.GeoSeries(bbox_to_shapely(val_roi)).boundary.plot(ax=ax, color=\"black\")\n",
    "plt.title(\n",
    "    f\"Train: {format_timestamp(train_roi.mint)} - {format_timestamp(train_roi.maxt)}\\n\"\n",
    "    f\"Val: {format_timestamp(val_roi.mint)} - {format_timestamp(val_roi.maxt)}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "We will train a Unet that receives the stacked NDVI rasters and outputs a crop segmentation map. To optimize our model, we employ pixel-wise cross-entropy loss between the network's prediction and the CDL map for the same chip.\n",
    "\n",
    "The `SegmentationModel` defines the network architecture and training parameters. It has the following arguments:\n",
    "\n",
    "- `lr`: learning rate.\n",
    "- `weight_decay`: amount of weight decay regularization.\n",
    "- `in_channels`: number of input channels of the network. Needs to match the number of bands/channels of the stacked NVDI raster.\n",
    "- `encoder_name`: name of the encoder used for the Unet. See segmentation_models_pytorch for more information. Default: 'resnet34'\n",
    "- `encoder_weights`: name of the pretrained weights for the encoder. Use 'imagenet' or None (random weights). See [segmentation_models_pytorch](https://smp.readthedocs.io/en/latest/index.html) for more information. Default: 'imagenet'\n",
    "- `classes`: number of output classes. As we are doing a binary crop vs. non-crop segmentation, we use the default value. Default: 1\n",
    "- `num_epochs`: number of training epochs. Used for the cosine annealing scheduler. Default: 10\n",
    "\n",
    "\n",
    "When training for specific crops you might want to tune the loss (specifically due to imbalance, the amount of positive pixels will be really small compared to negative ones). Feel free to play around with other parameters such as learning rate, weight decay, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegmentationModel(lr=LR, \n",
    "                        weight_decay=WD, \n",
    "                        in_channels=NDVI_STACK_BANDS,\n",
    "                        num_epochs=MAX_EPOCHS,\n",
    "                        classes=1,\n",
    "                        )\n",
    "\n",
    "# Default checkpoint callback will save a checkpoint at the end of every epoch\n",
    "callbacks = [ModelCheckpoint()]\n",
    "\n",
    "# Change to gpus=0 to run on CPU\n",
    "trainer = Trainer(max_epochs=MAX_EPOCHS, callbacks=callbacks, default_root_dir=ROOT_DIR, gpus = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We can train the model by calling the trainer's fit method and passing the lightning module and data module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if CHPT_PATH and os.path.exists(CHPT_PATH):\n",
    "    # Load model\n",
    "    model = SegmentationModel.load_from_checkpoint(CHPT_PATH)\n",
    "else:\n",
    "    # Train it now\n",
    "    trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We can evaluate the model by calling the validate method from the Pytorch-Lightning trainer. This will compute and print metrics for the validation set. Additionally, we plot the Precision-Recall curve, and display the operating point for a threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPs = model.val_metrics[\"ap\"].FPs\n",
    "TPs = model.val_metrics[\"ap\"].TPs\n",
    "FNs = model.val_metrics[\"ap\"].FNs\n",
    "\n",
    "Re = TPs / (TPs + FNs)\n",
    "Pr = TPs / (TPs + FPs)\n",
    "\n",
    "plt.plot(Re[0], Pr[0])\n",
    "plt.plot(Re[0, 50], Pr[0, 50], 'ro')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model to ONNX format\n",
    "\n",
    "[Open Neural Network Exchange](https://onnx.ai/) (ONNX) is a open source format that represents machine learning models, both deep learning and traditional ML. It is supported by many frameworks, tools and hardware, enabling interoperability between different components easily. Exporting our trained model in ONNX format allows us to load it and perform inference over new data under different hardware setups and even within FarmVibes.AI platform. For additional resources, refer to the [ONNX](https://onnx.ai/get-started.html) or [PyTorch](https://pytorch.org/docs/master/onnx.html) documentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Trace the model with sigmoid activation\n",
    "class ModelPlusSigmoid(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).sigmoid()\n",
    "\n",
    "# Set model to inference mode before exporting to ONNX\n",
    "trace_model = ModelPlusSigmoid(model).eval()    \n",
    "\n",
    "dummy_input = torch.randn(BATCH_SIZE, NDVI_STACK_BANDS, CHIP_SIZE, CHIP_SIZE)\n",
    "\n",
    "onnx_output_path = os.path.join(ROOT_DIR, \"crop_segmentation_model.onnx\")\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(trace_model,                                            \n",
    "                  dummy_input,                                           # model example input\n",
    "                  onnx_output_path,                                      # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,                                    # store the trained parameter weights inside the model file\n",
    "                  do_constant_folding=True,                              # whether to execute constant folding for optimization\n",
    "                  input_names=['ndvi_stack'],                            # the model's input names\n",
    "                  output_names=['seg_map'],                              # the model's output names\n",
    "                  dynamic_axes={'ndvi_stack' : {0 : 'batch_size'},       # variable length axes\n",
    "                                'seg_map' : {0 : 'batch_size'}}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use ONNX checker to verify the model was exported successfully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_output_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform inference in the cluster\n",
    "First, we need to add the model to the cluster. This can be done using the `farmvibes_ai.sh` script, for more information, see the [script documentation](../../documentation/FARMVIBES_SCRIPT.md).\n",
    "Files added via the script will be available to the worker at `/mnt/onnx-resources`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run command to add model to the cluster. You can also run this on a terminal\n",
    "!bash ../../farmvibes-ai.sh add-onnx $onnx_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.document_workflow(\"ml/crop_segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference in the validation region\n",
    "# inference_geom = gpd.GeoSeries(bbox_to_shapely(val_roi), crs=data.val_dataset.crs).to_crs(\"epsg:4326\").iloc[0]\n",
    "inference_run = client.run(\n",
    "    \"ml/crop_segmentation\",\n",
    "    \"Crop segmentation inference\",\n",
    "    geometry=geometry,\n",
    "    time_range=time_range,\n",
    "    # Set the path to our model\n",
    "    parameters={\"model_file\": \"/mnt/onnx-resources/crop_segmentation_model.onnx\"}\n",
    ")\n",
    "inference_run.monitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ground-truth data\n",
    "gt_ar = data.val_dataset[val_roi][\"mask\"][0].numpy()\n",
    "# Read inference results\n",
    "with rasterio.open(inference_run.output[\"segmentation\"][0].raster_asset.url) as src:\n",
    "    data_ar = src.read(1)\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(data_ar[:int(data_ar.shape[0]*.3)], interpolation=\"none\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(gt_ar, interpolation=\"none\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('eywa-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c3afbcd674f01dbe99f517668d03e48b91e064c61586fe19845e53435e7dd554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
