{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FarmVibes.AI Micro Climate Prediction\n",
    "\n",
    "This notebook demonstrates how to train a model to forecast weather. Current Notebook provided configuration to train model & inference for Temperature and WindSpeed.\n",
    "\n",
    "\n",
    "### Conda environment setup\n",
    "Before running this notebook, let's build a conda environment. If you do not have conda installed, please follow the instructions from [Conda User Guide](https://docs.conda.io/projects/conda/en/latest/user-guide/index.html). \n",
    "\n",
    "```\n",
    "$ conda env create -f ./deepmc_env.yaml\n",
    "$ conda activate deepmc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook outline\n",
    "Current script in Notebook configured to train model and inference weather parameters such as Temperature and WindSpeed. To execute notebook users must provide input data downloaded from a weather station. Required weather features to execute the notebook are datetime, humidity, windspeed and temperature. For model training, minimum 2 years of input historical data is   required, for inference 552 data points of historical data is required. \n",
    "\n",
    "\n",
    "Below are the main libraries used for this example and other useful links:\n",
    "- [Tensorflow](https://github.com/tensorflow/tensorflow) is used as our deep learning framework.\n",
    "- [Scikit-Learn](https://github.com/scikit-learn/scikit-learn) is a Python package for machine learning built on top of SciPy. It Simple and efficient tools for predictive data analysis.\n",
    "- [pandas](https://github.com/scikit-learn/scikit-learn) is a Python package that provides fast, flexible, and expressive data structures designed to make working with \"relational\" or \"labeled\" data both easy and intuitive.\n",
    "- [NumPy](https://github.com/numpy/numpy) is a python package that provides powerful N-dimensional array object, broadcasting functions and useful linear algebra, Fourier transform, and random number capabilities.\n",
    "- [pywt](https://github.com/PyWavelets/pywt) is a python package that provides mathematical basis functions that are localized in both time and frequency.\n",
    "\n",
    "\n",
    "### Code organization\n",
    "The training script rely on package in directory ./notebook_lib:\n",
    "\n",
    "- The code module (`notebook_lib/preprocess.py`) used to preprocess, transform the input data and bundle the input data into chunks that can be used for training Micro Climate Prediction model. For more information on wavelets (check [pywavelets documentation](https://pywavelets.readthedocs.io/en/latest/)).\n",
    "- The code module (`notebook_lib/models.py`, `notebook_lib/transformer_models_ts.py`, `notebook_lib/post_models.py`) contains the code that design the model with a set of tensorflow layers executed Sequentially. If you want to change the design of the model, understand the tensorflow layers, this is probably where you should go to. Also check [tensorflow documentation](https://www.tensorflow.org/learn).\n",
    "- The code module (`notebook_lib/train.py` and `notebook_lib/predict.py`) contains the code for running/training/evaluating the neural network: instantiating the neural network, training steps, computing metrics, and others. If you want to understand the mode performance, the loss, and generally how the model is trained, this is probably where you should go to. Also check [tensorflow documentation](https://www.tensorflow.org/learn).\n",
    "- The code module (`notebook_lib/utils.py`) contain commonly used functions to read csv files, scale and split data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "\n",
    "from notebook_lib import utils\n",
    "from notebook_lib import prediction\n",
    "from notebook_lib import preprocess\n",
    "from notebook_lib import train\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather dataset filtered and model training limited to train features.\n",
    "MODEL_TRAIN_FEATURES = ['humidity', 'wind_speed', 'temperature']\n",
    "\n",
    "# Models trained to predict out features\n",
    "MODEL_OUT_FEATURES = ['wind_speed', 'temperature']\n",
    "\n",
    "# Historical data aligned using INDEX variable\n",
    "INDEX = \"date\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGWeatherNet\n",
    "In this notebook, we utilize data downloaded from AGWeatherNet for a station \\\"Palouse\\\". The data used for training range from May 2020 to June 2022. For more information check [AGWeatherNet documentation](http://weather.wsu.edu/?p=92850&desktop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGWeatherNet station\n",
    "STATION_NAME = \"Palouse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The data downloaded from AGWeatherNet having the 15 minutes frequency. On data downloaded below preprocessing steps performed.\n",
    "\n",
    "1. The index variable converted to datetime\n",
    "2. The input data is interpolated to fill the missing values using the neighbors\n",
    "3. The script in Notebook focused on training the model with 60 minutes frequency, hence the data grouped to convert it to 60 minutes frequency.\n",
    "4. The data is scaled using the scikit-learn StandardScalar. For more information check [scikit-learn documentaion](https://github.com/scikit-learn/scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get csv data for a station\n",
    "file_path = f\"./data/{STATION_NAME}/training.csv\"\n",
    "predict=\"%s\"\n",
    "root_path = f\"./data/model_{predict}/\"\n",
    "data_export_path = root_path + \"train_data.pkl\"\n",
    "\n",
    "input_df = utils.get_csv_data(path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The script in notebook configured to train Micro Climate prediction model for 24 hours and actual weather station data points with 60 minutes frequency. Below inputs vary based on number of hours of prediction and frequency of weather station data points.\n",
    "\n",
    "1. `chunk_size` - The value of the chunk size based on frequency of weather station data points. For 60 minutes actual weather data frequency the minimum required data points are 528. If the data frequency is 15 minutes, the minimum number of data points required is 528*4 = 2112. These are minimum number of data points need to be provided as input during the inference.\n",
    "2. `ts_lookahead` - The value used during the data preprocessing. It's the value used to consider weather data points ahead for a given time period while grouping the data.\n",
    "3. `ts_lookback` - The value used during the data preprocessing. It's the value used to consider weather data points back for a given time period while grouping the data.\n",
    "4. `total_models` - To perform a 24 hour prediction with a weather data point having a frequency of 60 minutes, requires 24 models. One model for each 60 minutes. If number of hours of prediction to be increased then total number of data points are increased. \n",
    "5. `wavelet` - Wavelet object name used to perform discrete transformation of data. The current notebook configured to use `bior3.5`. For more information check [Discrete Wavelet Transform documentation](https://pywavelets.readthedocs.io/en/latest/ref/dwt-discrete-wavelet-transform.html)\n",
    "\n",
    "### Model Types\n",
    "The training process create two different types of models \n",
    "1. `Micro climate prediction model` - Used to predict the weather forecast. \n",
    "2. `Micro climate post-prediction model`- Scale the predicted weather forecast values using the training input data and reduce the error in prediction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weather = train.ModelTrainWeather(\n",
    "    train_features=MODEL_TRAIN_FEATURES,\n",
    "    out_features=MODEL_OUT_FEATURES,\n",
    "    root_path=root_path,\n",
    "    data_export_path=data_export_path,\n",
    "    station_name=STATION_NAME)\n",
    "\n",
    "train_weather.train_model(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Weather forecast\n",
    "The script in notebook configured to inference Micro Climate prediction model for 24 hours and actual weather station data points with 60 minutes frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"./data/{STATION_NAME}/prediction.csv\"\n",
    "\n",
    "input_df = utils.get_csv_data(path=file_path)\n",
    "\n",
    "base_data_df = input_df[MODEL_TRAIN_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_start_datetime = base_data_df.index[-1]\n",
    "\n",
    "df_output_merge = pd.DataFrame(columns=base_data_df.columns)\n",
    "\n",
    "weather_forecast = prediction.InferenceWeather(\n",
    "                        root_path=root_path,\n",
    "                        data_export_path=data_export_path,\n",
    "                        station_name=STATION_NAME,\n",
    "                        predicts=MODEL_OUT_FEATURES)\n",
    "\n",
    "df_out = weather_forecast.inference(base_data_df,\n",
    "            start_datetime=forecast_start_datetime\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for predict in MODEL_OUT_FEATURES:\n",
    "    # without using the scalar\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.plot(df_out[\"date\"].values, df_out[predict].values)\n",
    "    plt.title(\"24 Models Temperature Ground Truth Vs Predict\")\n",
    "    plt.legend([\"Predict\", \"Ground Truth\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('eywa-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3afbcd674f01dbe99f517668d03e48b91e064c61586fe19845e53435e7dd554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
